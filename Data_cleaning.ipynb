{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Processing for Car-Dheko Used Car Price Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sheets: ['bangalore_cars.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\USER\\Desktop\\Car-Dheko_Used_Car_Price_Prediction\\csv_files\\bangalore_cars.xlsx\"\n",
    "\n",
    "# Get all sheet names\n",
    "sheets = pd.ExcelFile(file_path).sheet_names\n",
    "print(\"Available sheets:\", sheets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      new_car_detail  \\\n",
      "0  {'it': 0, 'ft': 'Petrol', 'bt': 'Hatchback', '...   \n",
      "1  {'it': 0, 'ft': 'Petrol', 'bt': 'SUV', 'km': '...   \n",
      "2  {'it': 0, 'ft': 'Petrol', 'bt': 'Hatchback', '...   \n",
      "3  {'it': 0, 'ft': 'Petrol', 'bt': 'Sedan', 'km':...   \n",
      "4  {'it': 0, 'ft': 'Diesel', 'bt': 'SUV', 'km': '...   \n",
      "\n",
      "                                    new_car_overview  \\\n",
      "0  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "1  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "2  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "3  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "4  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "\n",
      "                                     new_car_feature  \\\n",
      "0  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "1  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "2  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "3  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "4  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "\n",
      "                                       new_car_specs  \\\n",
      "0  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "1  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "2  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "3  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "4  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "\n",
      "                                           car_links  \n",
      "0  https://www.cardekho.com/used-car-details/used...  \n",
      "1  https://www.cardekho.com/buy-used-car-details/...  \n",
      "2  https://www.cardekho.com/used-car-details/used...  \n",
      "3  https://www.cardekho.com/buy-used-car-details/...  \n",
      "4  https://www.cardekho.com/used-car-details/used...  \n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_excel(file_path, sheet_name=\"bangalore_cars.csv\")  # or sheet_name=0 for the first sheet\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Display Full Dictionary (No Truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car_links', 'detail_it', 'detail_ft', 'detail_bt', 'detail_km', 'detail_transmission', 'detail_ownerNo', 'detail_owner', 'detail_oem', 'detail_model', 'detail_modelYear', 'detail_centralVariantId', 'detail_variantName', 'detail_price', 'detail_priceActual', 'detail_priceSaving', 'detail_priceFixedText', 'detail_trendingText.imgUrl', 'detail_trendingText.heading', 'detail_trendingText.desc', 'new_car_overview_heading', 'new_car_overview_top', 'new_car_overview_bottomData', 'new_car_feature_heading', 'new_car_feature_top', 'new_car_feature_data', 'new_car_feature_commonIcon', 'new_car_specs_heading', 'new_car_specs_top', 'new_car_specs_data', 'new_car_specs_commonIcon', 'first_feature']\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       None\n",
      "1       None\n",
      "2       None\n",
      "3       None\n",
      "4       None\n",
      "5       None\n",
      "6       None\n",
      "7       None\n",
      "8       None\n",
      "9       None\n",
      "10      None\n",
      "11      None\n",
      "12      None\n",
      "13      None\n",
      "14      None\n",
      "15      None\n",
      "16      None\n",
      "17      None\n",
      "18      None\n",
      "19      None\n",
      "20      None\n",
      "21      None\n",
      "22      None\n",
      "23      None\n",
      "24      None\n",
      "25      None\n",
      "26      None\n",
      "27      None\n",
      "28      None\n",
      "29      None\n",
      "30      None\n",
      "31      None\n",
      "32      None\n",
      "33      None\n",
      "34      None\n",
      "35      None\n",
      "36      None\n",
      "37      None\n",
      "38      None\n",
      "39      None\n",
      "40      None\n",
      "41      None\n",
      "42      None\n",
      "43      None\n",
      "44      None\n",
      "45      None\n",
      "46      None\n",
      "47      None\n",
      "48      None\n",
      "49      None\n",
      "50      None\n",
      "51      None\n",
      "52      None\n",
      "53      None\n",
      "54      None\n",
      "55      None\n",
      "56      None\n",
      "57      None\n",
      "58      None\n",
      "59      None\n",
      "60      None\n",
      "61      None\n",
      "62      None\n",
      "63      None\n",
      "64      None\n",
      "65      None\n",
      "66      None\n",
      "67      None\n",
      "68      None\n",
      "69      None\n",
      "70      None\n",
      "71      None\n",
      "72      None\n",
      "73      None\n",
      "74      None\n",
      "75      None\n",
      "76      None\n",
      "77      None\n",
      "78      None\n",
      "79      None\n",
      "80      None\n",
      "81      None\n",
      "82      None\n",
      "83      None\n",
      "84      None\n",
      "85      None\n",
      "86      None\n",
      "87      None\n",
      "88      None\n",
      "89      None\n",
      "90      None\n",
      "91      None\n",
      "92      None\n",
      "93      None\n",
      "94      None\n",
      "95      None\n",
      "96      None\n",
      "97      None\n",
      "98      None\n",
      "99      None\n",
      "100     None\n",
      "101     None\n",
      "102     None\n",
      "103     None\n",
      "104     None\n",
      "105     None\n",
      "106     None\n",
      "107     None\n",
      "108     None\n",
      "109     None\n",
      "110     None\n",
      "111     None\n",
      "112     None\n",
      "113     None\n",
      "114     None\n",
      "115     None\n",
      "116     None\n",
      "117     None\n",
      "118     None\n",
      "119     None\n",
      "120     None\n",
      "121     None\n",
      "122     None\n",
      "123     None\n",
      "124     None\n",
      "125     None\n",
      "126     None\n",
      "127     None\n",
      "128     None\n",
      "129     None\n",
      "130     None\n",
      "131     None\n",
      "132     None\n",
      "133     None\n",
      "134     None\n",
      "135     None\n",
      "136     None\n",
      "137     None\n",
      "138     None\n",
      "139     None\n",
      "140     None\n",
      "141     None\n",
      "142     None\n",
      "143     None\n",
      "144     None\n",
      "145     None\n",
      "146     None\n",
      "147     None\n",
      "148     None\n",
      "149     None\n",
      "150     None\n",
      "151     None\n",
      "152     None\n",
      "153     None\n",
      "154     None\n",
      "155     None\n",
      "156     None\n",
      "157     None\n",
      "158     None\n",
      "159     None\n",
      "160     None\n",
      "161     None\n",
      "162     None\n",
      "163     None\n",
      "164     None\n",
      "165     None\n",
      "166     None\n",
      "167     None\n",
      "168     None\n",
      "169     None\n",
      "170     None\n",
      "171     None\n",
      "172     None\n",
      "173     None\n",
      "174     None\n",
      "175     None\n",
      "176     None\n",
      "177     None\n",
      "178     None\n",
      "179     None\n",
      "180     None\n",
      "181     None\n",
      "182     None\n",
      "183     None\n",
      "184     None\n",
      "185     None\n",
      "186     None\n",
      "187     None\n",
      "188     None\n",
      "189     None\n",
      "190     None\n",
      "191     None\n",
      "192     None\n",
      "193     None\n",
      "194     None\n",
      "195     None\n",
      "196     None\n",
      "197     None\n",
      "198     None\n",
      "199     None\n",
      "200     None\n",
      "201     None\n",
      "202     None\n",
      "203     None\n",
      "204     None\n",
      "205     None\n",
      "206     None\n",
      "207     None\n",
      "208     None\n",
      "209     None\n",
      "210     None\n",
      "211     None\n",
      "212     None\n",
      "213     None\n",
      "214     None\n",
      "215     None\n",
      "216     None\n",
      "217     None\n",
      "218     None\n",
      "219     None\n",
      "220     None\n",
      "221     None\n",
      "222     None\n",
      "223     None\n",
      "224     None\n",
      "225     None\n",
      "226     None\n",
      "227     None\n",
      "228     None\n",
      "229     None\n",
      "230     None\n",
      "231     None\n",
      "232     None\n",
      "233     None\n",
      "234     None\n",
      "235     None\n",
      "236     None\n",
      "237     None\n",
      "238     None\n",
      "239     None\n",
      "240     None\n",
      "241     None\n",
      "242     None\n",
      "243     None\n",
      "244     None\n",
      "245     None\n",
      "246     None\n",
      "247     None\n",
      "248     None\n",
      "249     None\n",
      "250     None\n",
      "251     None\n",
      "252     None\n",
      "253     None\n",
      "254     None\n",
      "255     None\n",
      "256     None\n",
      "257     None\n",
      "258     None\n",
      "259     None\n",
      "260     None\n",
      "261     None\n",
      "262     None\n",
      "263     None\n",
      "264     None\n",
      "265     None\n",
      "266     None\n",
      "267     None\n",
      "268     None\n",
      "269     None\n",
      "270     None\n",
      "271     None\n",
      "272     None\n",
      "273     None\n",
      "274     None\n",
      "275     None\n",
      "276     None\n",
      "277     None\n",
      "278     None\n",
      "279     None\n",
      "280     None\n",
      "281     None\n",
      "282     None\n",
      "283     None\n",
      "284     None\n",
      "285     None\n",
      "286     None\n",
      "287     None\n",
      "288     None\n",
      "289     None\n",
      "290     None\n",
      "291     None\n",
      "292     None\n",
      "293     None\n",
      "294     None\n",
      "295     None\n",
      "296     None\n",
      "297     None\n",
      "298     None\n",
      "299     None\n",
      "300     None\n",
      "301     None\n",
      "302     None\n",
      "303     None\n",
      "304     None\n",
      "305     None\n",
      "306     None\n",
      "307     None\n",
      "308     None\n",
      "309     None\n",
      "310     None\n",
      "311     None\n",
      "312     None\n",
      "313     None\n",
      "314     None\n",
      "315     None\n",
      "316     None\n",
      "317     None\n",
      "318     None\n",
      "319     None\n",
      "320     None\n",
      "321     None\n",
      "322     None\n",
      "323     None\n",
      "324     None\n",
      "325     None\n",
      "326     None\n",
      "327     None\n",
      "328     None\n",
      "329     None\n",
      "330     None\n",
      "331     None\n",
      "332     None\n",
      "333     None\n",
      "334     None\n",
      "335     None\n",
      "336     None\n",
      "337     None\n",
      "338     None\n",
      "339     None\n",
      "340     None\n",
      "341     None\n",
      "342     None\n",
      "343     None\n",
      "344     None\n",
      "345     None\n",
      "346     None\n",
      "347     None\n",
      "348     None\n",
      "349     None\n",
      "350     None\n",
      "351     None\n",
      "352     None\n",
      "353     None\n",
      "354     None\n",
      "355     None\n",
      "356     None\n",
      "357     None\n",
      "358     None\n",
      "359     None\n",
      "360     None\n",
      "361     None\n",
      "362     None\n",
      "363     None\n",
      "364     None\n",
      "365     None\n",
      "366     None\n",
      "367     None\n",
      "368     None\n",
      "369     None\n",
      "370     None\n",
      "371     None\n",
      "372     None\n",
      "373     None\n",
      "374     None\n",
      "375     None\n",
      "376     None\n",
      "377     None\n",
      "378     None\n",
      "379     None\n",
      "380     None\n",
      "381     None\n",
      "382     None\n",
      "383     None\n",
      "384     None\n",
      "385     None\n",
      "386     None\n",
      "387     None\n",
      "388     None\n",
      "389     None\n",
      "390     None\n",
      "391     None\n",
      "392     None\n",
      "393     None\n",
      "394     None\n",
      "395     None\n",
      "396     None\n",
      "397     None\n",
      "398     None\n",
      "399     None\n",
      "400     None\n",
      "401     None\n",
      "402     None\n",
      "403     None\n",
      "404     None\n",
      "405     None\n",
      "406     None\n",
      "407     None\n",
      "408     None\n",
      "409     None\n",
      "410     None\n",
      "411     None\n",
      "412     None\n",
      "413     None\n",
      "414     None\n",
      "415     None\n",
      "416     None\n",
      "417     None\n",
      "418     None\n",
      "419     None\n",
      "420     None\n",
      "421     None\n",
      "422     None\n",
      "423     None\n",
      "424     None\n",
      "425     None\n",
      "426     None\n",
      "427     None\n",
      "428     None\n",
      "429     None\n",
      "430     None\n",
      "431     None\n",
      "432     None\n",
      "433     None\n",
      "434     None\n",
      "435     None\n",
      "436     None\n",
      "437     None\n",
      "438     None\n",
      "439     None\n",
      "440     None\n",
      "441     None\n",
      "442     None\n",
      "443     None\n",
      "444     None\n",
      "445     None\n",
      "446     None\n",
      "447     None\n",
      "448     None\n",
      "449     None\n",
      "450     None\n",
      "451     None\n",
      "452     None\n",
      "453     None\n",
      "454     None\n",
      "455     None\n",
      "456     None\n",
      "457     None\n",
      "458     None\n",
      "459     None\n",
      "460     None\n",
      "461     None\n",
      "462     None\n",
      "463     None\n",
      "464     None\n",
      "465     None\n",
      "466     None\n",
      "467     None\n",
      "468     None\n",
      "469     None\n",
      "470     None\n",
      "471     None\n",
      "472     None\n",
      "473     None\n",
      "474     None\n",
      "475     None\n",
      "476     None\n",
      "477     None\n",
      "478     None\n",
      "479     None\n",
      "480     None\n",
      "481     None\n",
      "482     None\n",
      "483     None\n",
      "484     None\n",
      "485     None\n",
      "486     None\n",
      "487     None\n",
      "488     None\n",
      "489     None\n",
      "490     None\n",
      "491     None\n",
      "492     None\n",
      "493     None\n",
      "494     None\n",
      "495     None\n",
      "496     None\n",
      "497     None\n",
      "498     None\n",
      "499     None\n",
      "500     None\n",
      "501     None\n",
      "502     None\n",
      "503     None\n",
      "504     None\n",
      "505     None\n",
      "506     None\n",
      "507     None\n",
      "508     None\n",
      "509     None\n",
      "510     None\n",
      "511     None\n",
      "512     None\n",
      "513     None\n",
      "514     None\n",
      "515     None\n",
      "516     None\n",
      "517     None\n",
      "518     None\n",
      "519     None\n",
      "520     None\n",
      "521     None\n",
      "522     None\n",
      "523     None\n",
      "524     None\n",
      "525     None\n",
      "526     None\n",
      "527     None\n",
      "528     None\n",
      "529     None\n",
      "530     None\n",
      "531     None\n",
      "532     None\n",
      "533     None\n",
      "534     None\n",
      "535     None\n",
      "536     None\n",
      "537     None\n",
      "538     None\n",
      "539     None\n",
      "540     None\n",
      "541     None\n",
      "542     None\n",
      "543     None\n",
      "544     None\n",
      "545     None\n",
      "546     None\n",
      "547     None\n",
      "548     None\n",
      "549     None\n",
      "550     None\n",
      "551     None\n",
      "552     None\n",
      "553     None\n",
      "554     None\n",
      "555     None\n",
      "556     None\n",
      "557     None\n",
      "558     None\n",
      "559     None\n",
      "560     None\n",
      "561     None\n",
      "562     None\n",
      "563     None\n",
      "564     None\n",
      "565     None\n",
      "566     None\n",
      "567     None\n",
      "568     None\n",
      "569     None\n",
      "570     None\n",
      "571     None\n",
      "572     None\n",
      "573     None\n",
      "574     None\n",
      "575     None\n",
      "576     None\n",
      "577     None\n",
      "578     None\n",
      "579     None\n",
      "580     None\n",
      "581     None\n",
      "582     None\n",
      "583     None\n",
      "584     None\n",
      "585     None\n",
      "586     None\n",
      "587     None\n",
      "588     None\n",
      "589     None\n",
      "590     None\n",
      "591     None\n",
      "592     None\n",
      "593     None\n",
      "594     None\n",
      "595     None\n",
      "596     None\n",
      "597     None\n",
      "598     None\n",
      "599     None\n",
      "600     None\n",
      "601     None\n",
      "602     None\n",
      "603     None\n",
      "604     None\n",
      "605     None\n",
      "606     None\n",
      "607     None\n",
      "608     None\n",
      "609     None\n",
      "610     None\n",
      "611     None\n",
      "612     None\n",
      "613     None\n",
      "614     None\n",
      "615     None\n",
      "616     None\n",
      "617     None\n",
      "618     None\n",
      "619     None\n",
      "620     None\n",
      "621     None\n",
      "622     None\n",
      "623     None\n",
      "624     None\n",
      "625     None\n",
      "626     None\n",
      "627     None\n",
      "628     None\n",
      "629     None\n",
      "630     None\n",
      "631     None\n",
      "632     None\n",
      "633     None\n",
      "634     None\n",
      "635     None\n",
      "636     None\n",
      "637     None\n",
      "638     None\n",
      "639     None\n",
      "640     None\n",
      "641     None\n",
      "642     None\n",
      "643     None\n",
      "644     None\n",
      "645     None\n",
      "646     None\n",
      "647     None\n",
      "648     None\n",
      "649     None\n",
      "650     None\n",
      "651     None\n",
      "652     None\n",
      "653     None\n",
      "654     None\n",
      "655     None\n",
      "656     None\n",
      "657     None\n",
      "658     None\n",
      "659     None\n",
      "660     None\n",
      "661     None\n",
      "662     None\n",
      "663     None\n",
      "664     None\n",
      "665     None\n",
      "666     None\n",
      "667     None\n",
      "668     None\n",
      "669     None\n",
      "670     None\n",
      "671     None\n",
      "672     None\n",
      "673     None\n",
      "674     None\n",
      "675     None\n",
      "676     None\n",
      "677     None\n",
      "678     None\n",
      "679     None\n",
      "680     None\n",
      "681     None\n",
      "682     None\n",
      "683     None\n",
      "684     None\n",
      "685     None\n",
      "686     None\n",
      "687     None\n",
      "688     None\n",
      "689     None\n",
      "690     None\n",
      "691     None\n",
      "692     None\n",
      "693     None\n",
      "694     None\n",
      "695     None\n",
      "696     None\n",
      "697     None\n",
      "698     None\n",
      "699     None\n",
      "700     None\n",
      "701     None\n",
      "702     None\n",
      "703     None\n",
      "704     None\n",
      "705     None\n",
      "706     None\n",
      "707     None\n",
      "708     None\n",
      "709     None\n",
      "710     None\n",
      "711     None\n",
      "712     None\n",
      "713     None\n",
      "714     None\n",
      "715     None\n",
      "716     None\n",
      "717     None\n",
      "718     None\n",
      "719     None\n",
      "720     None\n",
      "721     None\n",
      "722     None\n",
      "723     None\n",
      "724     None\n",
      "725     None\n",
      "726     None\n",
      "727     None\n",
      "728     None\n",
      "729     None\n",
      "730     None\n",
      "731     None\n",
      "732     None\n",
      "733     None\n",
      "734     None\n",
      "735     None\n",
      "736     None\n",
      "737     None\n",
      "738     None\n",
      "739     None\n",
      "740     None\n",
      "741     None\n",
      "742     None\n",
      "743     None\n",
      "744     None\n",
      "745     None\n",
      "746     None\n",
      "747     None\n",
      "748     None\n",
      "749     None\n",
      "750     None\n",
      "751     None\n",
      "752     None\n",
      "753     None\n",
      "754     None\n",
      "755     None\n",
      "756     None\n",
      "757     None\n",
      "758     None\n",
      "759     None\n",
      "760     None\n",
      "761     None\n",
      "762     None\n",
      "763     None\n",
      "764     None\n",
      "765     None\n",
      "766     None\n",
      "767     None\n",
      "768     None\n",
      "769     None\n",
      "770     None\n",
      "771     None\n",
      "772     None\n",
      "773     None\n",
      "774     None\n",
      "775     None\n",
      "776     None\n",
      "777     None\n",
      "778     None\n",
      "779     None\n",
      "780     None\n",
      "781     None\n",
      "782     None\n",
      "783     None\n",
      "784     None\n",
      "785     None\n",
      "786     None\n",
      "787     None\n",
      "788     None\n",
      "789     None\n",
      "790     None\n",
      "791     None\n",
      "792     None\n",
      "793     None\n",
      "794     None\n",
      "795     None\n",
      "796     None\n",
      "797     None\n",
      "798     None\n",
      "799     None\n",
      "800     None\n",
      "801     None\n",
      "802     None\n",
      "803     None\n",
      "804     None\n",
      "805     None\n",
      "806     None\n",
      "807     None\n",
      "808     None\n",
      "809     None\n",
      "810     None\n",
      "811     None\n",
      "812     None\n",
      "813     None\n",
      "814     None\n",
      "815     None\n",
      "816     None\n",
      "817     None\n",
      "818     None\n",
      "819     None\n",
      "820     None\n",
      "821     None\n",
      "822     None\n",
      "823     None\n",
      "824     None\n",
      "825     None\n",
      "826     None\n",
      "827     None\n",
      "828     None\n",
      "829     None\n",
      "830     None\n",
      "831     None\n",
      "832     None\n",
      "833     None\n",
      "834     None\n",
      "835     None\n",
      "836     None\n",
      "837     None\n",
      "838     None\n",
      "839     None\n",
      "840     None\n",
      "841     None\n",
      "842     None\n",
      "843     None\n",
      "844     None\n",
      "845     None\n",
      "846     None\n",
      "847     None\n",
      "848     None\n",
      "849     None\n",
      "850     None\n",
      "851     None\n",
      "852     None\n",
      "853     None\n",
      "854     None\n",
      "855     None\n",
      "856     None\n",
      "857     None\n",
      "858     None\n",
      "859     None\n",
      "860     None\n",
      "861     None\n",
      "862     None\n",
      "863     None\n",
      "864     None\n",
      "865     None\n",
      "866     None\n",
      "867     None\n",
      "868     None\n",
      "869     None\n",
      "870     None\n",
      "871     None\n",
      "872     None\n",
      "873     None\n",
      "874     None\n",
      "875     None\n",
      "876     None\n",
      "877     None\n",
      "878     None\n",
      "879     None\n",
      "880     None\n",
      "881     None\n",
      "882     None\n",
      "883     None\n",
      "884     None\n",
      "885     None\n",
      "886     None\n",
      "887     None\n",
      "888     None\n",
      "889     None\n",
      "890     None\n",
      "891     None\n",
      "892     None\n",
      "893     None\n",
      "894     None\n",
      "895     None\n",
      "896     None\n",
      "897     None\n",
      "898     None\n",
      "899     None\n",
      "900     None\n",
      "901     None\n",
      "902     None\n",
      "903     None\n",
      "904     None\n",
      "905     None\n",
      "906     None\n",
      "907     None\n",
      "908     None\n",
      "909     None\n",
      "910     None\n",
      "911     None\n",
      "912     None\n",
      "913     None\n",
      "914     None\n",
      "915     None\n",
      "916     None\n",
      "917     None\n",
      "918     None\n",
      "919     None\n",
      "920     None\n",
      "921     None\n",
      "922     None\n",
      "923     None\n",
      "924     None\n",
      "925     None\n",
      "926     None\n",
      "927     None\n",
      "928     None\n",
      "929     None\n",
      "930     None\n",
      "931     None\n",
      "932     None\n",
      "933     None\n",
      "934     None\n",
      "935     None\n",
      "936     None\n",
      "937     None\n",
      "938     None\n",
      "939     None\n",
      "940     None\n",
      "941     None\n",
      "942     None\n",
      "943     None\n",
      "944     None\n",
      "945     None\n",
      "946     None\n",
      "947     None\n",
      "948     None\n",
      "949     None\n",
      "950     None\n",
      "951     None\n",
      "952     None\n",
      "953     None\n",
      "954     None\n",
      "955     None\n",
      "956     None\n",
      "957     None\n",
      "958     None\n",
      "959     None\n",
      "960     None\n",
      "961     None\n",
      "962     None\n",
      "963     None\n",
      "964     None\n",
      "965     None\n",
      "966     None\n",
      "967     None\n",
      "968     None\n",
      "969     None\n",
      "970     None\n",
      "971     None\n",
      "972     None\n",
      "973     None\n",
      "974     None\n",
      "975     None\n",
      "976     None\n",
      "977     None\n",
      "978     None\n",
      "979     None\n",
      "980     None\n",
      "981     None\n",
      "982     None\n",
      "983     None\n",
      "984     None\n",
      "985     None\n",
      "986     None\n",
      "987     None\n",
      "988     None\n",
      "989     None\n",
      "990     None\n",
      "991     None\n",
      "992     None\n",
      "993     None\n",
      "994     None\n",
      "995     None\n",
      "996     None\n",
      "997     None\n",
      "998     None\n",
      "999     None\n",
      "1000    None\n",
      "1001    None\n",
      "1002    None\n",
      "1003    None\n",
      "1004    None\n",
      "1005    None\n",
      "1006    None\n",
      "1007    None\n",
      "1008    None\n",
      "1009    None\n",
      "1010    None\n",
      "1011    None\n",
      "1012    None\n",
      "1013    None\n",
      "1014    None\n",
      "1015    None\n",
      "1016    None\n",
      "1017    None\n",
      "1018    None\n",
      "1019    None\n",
      "1020    None\n",
      "1021    None\n",
      "1022    None\n",
      "1023    None\n",
      "1024    None\n",
      "1025    None\n",
      "1026    None\n",
      "1027    None\n",
      "1028    None\n",
      "1029    None\n",
      "1030    None\n",
      "1031    None\n",
      "1032    None\n",
      "1033    None\n",
      "1034    None\n",
      "1035    None\n",
      "1036    None\n",
      "1037    None\n",
      "1038    None\n",
      "1039    None\n",
      "1040    None\n",
      "1041    None\n",
      "1042    None\n",
      "1043    None\n",
      "1044    None\n",
      "1045    None\n",
      "1046    None\n",
      "1047    None\n",
      "1048    None\n",
      "1049    None\n",
      "1050    None\n",
      "1051    None\n",
      "1052    None\n",
      "1053    None\n",
      "1054    None\n",
      "1055    None\n",
      "1056    None\n",
      "1057    None\n",
      "1058    None\n",
      "1059    None\n",
      "1060    None\n",
      "1061    None\n",
      "1062    None\n",
      "1063    None\n",
      "1064    None\n",
      "1065    None\n",
      "1066    None\n",
      "1067    None\n",
      "1068    None\n",
      "1069    None\n",
      "1070    None\n",
      "1071    None\n",
      "1072    None\n",
      "1073    None\n",
      "1074    None\n",
      "1075    None\n",
      "1076    None\n",
      "1077    None\n",
      "1078    None\n",
      "1079    None\n",
      "1080    None\n",
      "1081    None\n",
      "1082    None\n",
      "1083    None\n",
      "1084    None\n",
      "1085    None\n",
      "1086    None\n",
      "1087    None\n",
      "1088    None\n",
      "1089    None\n",
      "1090    None\n",
      "1091    None\n",
      "1092    None\n",
      "1093    None\n",
      "1094    None\n",
      "1095    None\n",
      "1096    None\n",
      "1097    None\n",
      "1098    None\n",
      "1099    None\n",
      "1100    None\n",
      "1101    None\n",
      "1102    None\n",
      "1103    None\n",
      "1104    None\n",
      "1105    None\n",
      "1106    None\n",
      "1107    None\n",
      "1108    None\n",
      "1109    None\n",
      "1110    None\n",
      "1111    None\n",
      "1112    None\n",
      "1113    None\n",
      "1114    None\n",
      "1115    None\n",
      "1116    None\n",
      "1117    None\n",
      "1118    None\n",
      "1119    None\n",
      "1120    None\n",
      "1121    None\n",
      "1122    None\n",
      "1123    None\n",
      "1124    None\n",
      "1125    None\n",
      "1126    None\n",
      "1127    None\n",
      "1128    None\n",
      "1129    None\n",
      "1130    None\n",
      "1131    None\n",
      "1132    None\n",
      "1133    None\n",
      "1134    None\n",
      "1135    None\n",
      "1136    None\n",
      "1137    None\n",
      "1138    None\n",
      "1139    None\n",
      "1140    None\n",
      "1141    None\n",
      "1142    None\n",
      "1143    None\n",
      "1144    None\n",
      "1145    None\n",
      "1146    None\n",
      "1147    None\n",
      "1148    None\n",
      "1149    None\n",
      "1150    None\n",
      "1151    None\n",
      "1152    None\n",
      "1153    None\n",
      "1154    None\n",
      "1155    None\n",
      "1156    None\n",
      "1157    None\n",
      "1158    None\n",
      "1159    None\n",
      "1160    None\n",
      "1161    None\n",
      "1162    None\n",
      "1163    None\n",
      "1164    None\n",
      "1165    None\n",
      "1166    None\n",
      "1167    None\n",
      "1168    None\n",
      "1169    None\n",
      "1170    None\n",
      "1171    None\n",
      "1172    None\n",
      "1173    None\n",
      "1174    None\n",
      "1175    None\n",
      "1176    None\n",
      "1177    None\n",
      "1178    None\n",
      "1179    None\n",
      "1180    None\n",
      "1181    None\n",
      "1182    None\n",
      "1183    None\n",
      "1184    None\n",
      "1185    None\n",
      "1186    None\n",
      "1187    None\n",
      "1188    None\n",
      "1189    None\n",
      "1190    None\n",
      "1191    None\n",
      "1192    None\n",
      "1193    None\n",
      "1194    None\n",
      "1195    None\n",
      "1196    None\n",
      "1197    None\n",
      "1198    None\n",
      "1199    None\n",
      "1200    None\n",
      "1201    None\n",
      "1202    None\n",
      "1203    None\n",
      "1204    None\n",
      "1205    None\n",
      "1206    None\n",
      "1207    None\n",
      "1208    None\n",
      "1209    None\n",
      "1210    None\n",
      "1211    None\n",
      "1212    None\n",
      "1213    None\n",
      "1214    None\n",
      "1215    None\n",
      "1216    None\n",
      "1217    None\n",
      "1218    None\n",
      "1219    None\n",
      "1220    None\n",
      "1221    None\n",
      "1222    None\n",
      "1223    None\n",
      "1224    None\n",
      "1225    None\n",
      "1226    None\n",
      "1227    None\n",
      "1228    None\n",
      "1229    None\n",
      "1230    None\n",
      "1231    None\n",
      "1232    None\n",
      "1233    None\n",
      "1234    None\n",
      "1235    None\n",
      "1236    None\n",
      "1237    None\n",
      "1238    None\n",
      "1239    None\n",
      "1240    None\n",
      "1241    None\n",
      "1242    None\n",
      "1243    None\n",
      "1244    None\n",
      "1245    None\n",
      "1246    None\n",
      "1247    None\n",
      "1248    None\n",
      "1249    None\n",
      "1250    None\n",
      "1251    None\n",
      "1252    None\n",
      "1253    None\n",
      "1254    None\n",
      "1255    None\n",
      "1256    None\n",
      "1257    None\n",
      "1258    None\n",
      "1259    None\n",
      "1260    None\n",
      "1261    None\n",
      "1262    None\n",
      "1263    None\n",
      "1264    None\n",
      "1265    None\n",
      "1266    None\n",
      "1267    None\n",
      "1268    None\n",
      "1269    None\n",
      "1270    None\n",
      "1271    None\n",
      "1272    None\n",
      "1273    None\n",
      "1274    None\n",
      "1275    None\n",
      "1276    None\n",
      "1277    None\n",
      "1278    None\n",
      "1279    None\n",
      "1280    None\n",
      "1281    None\n",
      "1282    None\n",
      "1283    None\n",
      "1284    None\n",
      "1285    None\n",
      "1286    None\n",
      "1287    None\n",
      "1288    None\n",
      "1289    None\n",
      "1290    None\n",
      "1291    None\n",
      "1292    None\n",
      "1293    None\n",
      "1294    None\n",
      "1295    None\n",
      "1296    None\n",
      "1297    None\n",
      "1298    None\n",
      "1299    None\n",
      "1300    None\n",
      "1301    None\n",
      "1302    None\n",
      "1303    None\n",
      "1304    None\n",
      "1305    None\n",
      "1306    None\n",
      "1307    None\n",
      "1308    None\n",
      "1309    None\n",
      "1310    None\n",
      "1311    None\n",
      "1312    None\n",
      "1313    None\n",
      "1314    None\n",
      "1315    None\n",
      "1316    None\n",
      "1317    None\n",
      "1318    None\n",
      "1319    None\n",
      "1320    None\n",
      "1321    None\n",
      "1322    None\n",
      "1323    None\n",
      "1324    None\n",
      "1325    None\n",
      "1326    None\n",
      "1327    None\n",
      "1328    None\n",
      "1329    None\n",
      "1330    None\n",
      "1331    None\n",
      "1332    None\n",
      "1333    None\n",
      "1334    None\n",
      "1335    None\n",
      "1336    None\n",
      "1337    None\n",
      "1338    None\n",
      "1339    None\n",
      "1340    None\n",
      "1341    None\n",
      "1342    None\n",
      "1343    None\n",
      "1344    None\n",
      "1345    None\n",
      "1346    None\n",
      "1347    None\n",
      "1348    None\n",
      "1349    None\n",
      "1350    None\n",
      "1351    None\n",
      "1352    None\n",
      "1353    None\n",
      "1354    None\n",
      "1355    None\n",
      "1356    None\n",
      "1357    None\n",
      "1358    None\n",
      "1359    None\n",
      "1360    None\n",
      "1361    None\n",
      "1362    None\n",
      "1363    None\n",
      "1364    None\n",
      "1365    None\n",
      "1366    None\n",
      "1367    None\n",
      "1368    None\n",
      "1369    None\n",
      "1370    None\n",
      "1371    None\n",
      "1372    None\n",
      "1373    None\n",
      "1374    None\n",
      "1375    None\n",
      "1376    None\n",
      "1377    None\n",
      "1378    None\n",
      "1379    None\n",
      "1380    None\n",
      "1381    None\n",
      "1382    None\n",
      "1383    None\n",
      "1384    None\n",
      "1385    None\n",
      "1386    None\n",
      "1387    None\n",
      "1388    None\n",
      "1389    None\n",
      "1390    None\n",
      "1391    None\n",
      "1392    None\n",
      "1393    None\n",
      "1394    None\n",
      "1395    None\n",
      "1396    None\n",
      "1397    None\n",
      "1398    None\n",
      "1399    None\n",
      "1400    None\n",
      "1401    None\n",
      "1402    None\n",
      "1403    None\n",
      "1404    None\n",
      "1405    None\n",
      "1406    None\n",
      "1407    None\n",
      "1408    None\n",
      "1409    None\n",
      "1410    None\n",
      "1411    None\n",
      "1412    None\n",
      "1413    None\n",
      "1414    None\n",
      "1415    None\n",
      "1416    None\n",
      "1417    None\n",
      "1418    None\n",
      "1419    None\n",
      "1420    None\n",
      "1421    None\n",
      "1422    None\n",
      "1423    None\n",
      "1424    None\n",
      "1425    None\n",
      "1426    None\n",
      "1427    None\n",
      "1428    None\n",
      "1429    None\n",
      "1430    None\n",
      "1431    None\n",
      "1432    None\n",
      "1433    None\n",
      "1434    None\n",
      "1435    None\n",
      "1436    None\n",
      "1437    None\n",
      "1438    None\n",
      "1439    None\n",
      "1440    None\n",
      "1441    None\n",
      "1442    None\n",
      "1443    None\n",
      "1444    None\n",
      "1445    None\n",
      "1446    None\n",
      "1447    None\n",
      "1448    None\n",
      "1449    None\n",
      "1450    None\n",
      "1451    None\n",
      "1452    None\n",
      "1453    None\n",
      "1454    None\n",
      "1455    None\n",
      "1456    None\n",
      "1457    None\n",
      "1458    None\n",
      "1459    None\n",
      "1460    None\n",
      "1461    None\n",
      "1462    None\n",
      "1463    None\n",
      "1464    None\n",
      "1465    None\n",
      "1466    None\n",
      "1467    None\n",
      "1468    None\n",
      "1469    None\n",
      "1470    None\n",
      "1471    None\n",
      "1472    None\n",
      "1473    None\n",
      "1474    None\n",
      "1475    None\n",
      "1476    None\n",
      "1477    None\n",
      "1478    None\n",
      "1479    None\n",
      "1480    None\n",
      "Name: new_car_overview_bottomData, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Set Pandas to show full content of dictionaries\n",
    "pd.set_option('display.max_colwidth', None)  # Disable column width truncation\n",
    "\n",
    "# Display the 'new_car_detail' column\n",
    "print(df1['new_car_overview_bottomData'].apply(lambda x: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1481 entries, 0 to 1480\n",
      "Data columns (total 32 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   car_links                    1481 non-null   object\n",
      " 1   detail_it                    1481 non-null   int64 \n",
      " 2   detail_ft                    1481 non-null   object\n",
      " 3   detail_bt                    1481 non-null   object\n",
      " 4   detail_km                    1481 non-null   object\n",
      " 5   detail_transmission          1481 non-null   object\n",
      " 6   detail_ownerNo               1481 non-null   int64 \n",
      " 7   detail_owner                 1481 non-null   object\n",
      " 8   detail_oem                   1481 non-null   object\n",
      " 9   detail_model                 1481 non-null   object\n",
      " 10  detail_modelYear             1481 non-null   int64 \n",
      " 11  detail_centralVariantId      1481 non-null   int64 \n",
      " 12  detail_variantName           1481 non-null   object\n",
      " 13  detail_price                 1481 non-null   object\n",
      " 14  detail_priceActual           1481 non-null   object\n",
      " 15  detail_priceSaving           1481 non-null   object\n",
      " 16  detail_priceFixedText        0 non-null      object\n",
      " 17  detail_trendingText.imgUrl   1481 non-null   object\n",
      " 18  detail_trendingText.heading  1481 non-null   object\n",
      " 19  detail_trendingText.desc     1481 non-null   object\n",
      " 20  new_car_overview_heading     1481 non-null   object\n",
      " 21  new_car_overview_top         1481 non-null   object\n",
      " 22  new_car_overview_bottomData  0 non-null      object\n",
      " 23  new_car_feature_heading      1481 non-null   object\n",
      " 24  new_car_feature_top          1481 non-null   object\n",
      " 25  new_car_feature_data         1481 non-null   object\n",
      " 26  new_car_feature_commonIcon   1481 non-null   object\n",
      " 27  new_car_specs_heading        1481 non-null   object\n",
      " 28  new_car_specs_top            1481 non-null   object\n",
      " 29  new_car_specs_data           1481 non-null   object\n",
      " 30  new_car_specs_commonIcon     1481 non-null   object\n",
      " 31  first_feature                1467 non-null   object\n",
      "dtypes: int64(4), object(28)\n",
      "memory usage: 370.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Convert JSON strings to Python dictionaries\n",
    "First, ensure the columns contain actual dictionaries (not strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert stringified JSON to dict (if needed)\n",
    "for col in ['new_car_detail', 'new_car_overview', 'new_car_feature', 'new_car_specs']:\n",
    "    if isinstance(df1[col].iloc[0], str):\n",
    "        df1[col] = df1[col].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Flatten nested columns\n",
    "We'll use json_normalize to unpack dictionaries into separate columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize\n",
    "\n",
    "# Flatten the 'new_car_detail' column\n",
    "detail_cols = json_normalize(df1['new_car_detail'])\n",
    "detail_cols.columns = [f\"detail_{col}\" for col in detail_cols.columns]\n",
    "df1 = pd.concat([df1, detail_cols], axis=1).drop('new_car_detail', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Flatten other nested columns\n",
    "Repeat for other columns (adjust field names as needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['new_car_overview', 'new_car_feature', 'new_car_specs']:\n",
    "    flattened = json_normalize(df1[col])\n",
    "    flattened.columns = [f\"{col}_{subcol}\" for subcol in flattened.columns]\n",
    "    df1 = pd.concat([df1, flattened], axis=1).drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: View ALL Expanded Data\n",
    "To see all rows and columns without truncation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           car_links  detail_it detail_ft  detail_bt detail_km detail_transmission  detail_ownerNo detail_owner detail_oem        detail_model  detail_modelYear  detail_centralVariantId        detail_variantName detail_price detail_priceActual detail_priceSaving detail_priceFixedText                         detail_trendingText.imgUrl detail_trendingText.heading             detail_trendingText.desc new_car_overview_heading                               new_car_overview_top new_car_overview_bottomData new_car_feature_heading                                new_car_feature_top                               new_car_feature_data                         new_car_feature_commonIcon new_car_specs_heading                                  new_car_specs_top                                 new_car_specs_data new_car_specs_commonIcon\n",
      "0  https://www.cardekho.com/used-car-details/used...          0    Petrol  Hatchback  1,20,000              Manual               3    3rd Owner     Maruti      Maruti Celerio              2015                     3979                       VXI     ₹ 4 Lakh                                                        None  https://stimg.cardekho.com/used-cars/common/ic...               Trending Car!  High chances of sale in next 6 days             Car overview  [{'key': 'Registration Year', 'value': '2015',...                        None                Features  [{'value': 'Power Steering'}, {'value': 'Power...  [{'heading': 'Comfort & Convenience', 'subHead...  https://stimg.cardekho.com/pwa/img/vdpN/tickG.svg        Specifications  [{'key': 'Mileage', 'value': '23.1 kmpl'}, {'k...  [{'heading': 'Engine and Transmission', 'subHe...                         \n",
      "1  https://www.cardekho.com/buy-used-car-details/...          0    Petrol        SUV    32,706              Manual               2    2nd Owner       Ford       Ford Ecosport              2018                     6087  1.5 Petrol Titanium BSIV  ₹ 8.11 Lakh                                                        None  https://stimg.cardekho.com/used-cars/common/ic...               Trending Car!  High chances of sale in next 6 days             Car overview  [{'key': 'Registration Year', 'value': 'Feb 20...                        None                Features  [{'value': 'Power Steering'}, {'value': 'Power...  [{'heading': 'Comfort & Convenience', 'subHead...  https://stimg.cardekho.com/pwa/img/vdpN/tickG.svg        Specifications  [{'key': 'Mileage', 'value': '17 kmpl'}, {'key...  [{'heading': 'Engine and Transmission', 'subHe...                         \n",
      "2  https://www.cardekho.com/used-car-details/used...          0    Petrol  Hatchback    11,949              Manual               1    1st Owner       Tata          Tata Tiago              2018                     2983           1.2 Revotron XZ  ₹ 5.85 Lakh                                                        None  https://stimg.cardekho.com/used-cars/common/ic...               Trending Car!  High chances of sale in next 6 days             Car overview  [{'key': 'Registration Year', 'value': 'Sept 2...                        None                Features  [{'value': 'Power Steering'}, {'value': 'Power...  [{'heading': 'Comfort & Convenience', 'subHead...  https://stimg.cardekho.com/pwa/img/vdpN/tickG.svg        Specifications  [{'key': 'Mileage', 'value': '23.84 kmpl'}, {'...  [{'heading': 'Engine and Transmission', 'subHe...                         \n",
      "3  https://www.cardekho.com/buy-used-car-details/...          0    Petrol      Sedan    17,794              Manual               1    1st Owner    Hyundai       Hyundai Xcent              2014                     1867        1.2 Kappa S Option  ₹ 4.62 Lakh                                                        None  https://stimg.cardekho.com/used-cars/common/ic...               Trending Car!  High chances of sale in next 6 days             Car overview  [{'key': 'Registration Year', 'value': 'Dec 20...                        None                Features  [{'value': 'Power Steering'}, {'value': 'Power...  [{'heading': 'Comfort & Convenience', 'subHead...  https://stimg.cardekho.com/pwa/img/vdpN/tickG.svg        Specifications  [{'key': 'Mileage', 'value': '19.1 kmpl'}, {'k...  [{'heading': 'Engine and Transmission', 'subHe...                         \n",
      "4  https://www.cardekho.com/used-car-details/used...          0    Diesel        SUV    60,000              Manual               1    1st Owner     Maruti  Maruti SX4 S Cross              2015                     4277             DDiS 200 Zeta  ₹ 7.90 Lakh                                                        None  https://stimg.cardekho.com/used-cars/common/ic...               Trending Car!  High chances of sale in next 6 days             Car overview  [{'key': 'Registration Year', 'value': '2015',...                        None                Features  [{'value': 'Power Steering'}, {'value': 'Power...  [{'heading': 'Comfort & Convenience', 'subHead...  https://stimg.cardekho.com/pwa/img/vdpN/tickG.svg        Specifications  [{'key': 'Mileage', 'value': '23.65 kmpl'}, {'...  [{'heading': 'Engine and Transmission', 'subHe...                         \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 50)  # Adjust column width\n",
    "\n",
    "print(df1.head())  # Now shows full expanded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract first feature\n",
    "df1['first_feature'] = df1['new_car_feature_top'].apply(lambda x: x[0]['value'] if x else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been structured and saved to 'structured_bangalore_cars.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"csv_files/bangalore_cars.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"bangalore_cars.csv\")\n",
    "\n",
    "# Function to convert string to dictionary\n",
    "def parse_dict_column(column):\n",
    "    return column.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Parse each dictionary column\n",
    "df[\"new_car_detail\"] = parse_dict_column(df[\"new_car_detail\"])\n",
    "df[\"new_car_overview\"] = parse_dict_column(df[\"new_car_overview\"])\n",
    "df[\"new_car_feature\"] = parse_dict_column(df[\"new_car_feature\"])\n",
    "df[\"new_car_specs\"] = parse_dict_column(df[\"new_car_specs\"])\n",
    "\n",
    "# Normalize and flatten each dictionary\n",
    "df_banglore_detail = json_normalize(df[\"new_car_detail\"])\n",
    "df_banglore_overview = json_normalize(df[\"new_car_overview\"].apply(lambda x: {item['key']: item['value'] for item in x.get('top', [])}))\n",
    "df_banglore_feature = json_normalize(df[\"new_car_feature\"].apply(lambda x: {'Feature_' + str(i): f['value'] for i, f in enumerate(x.get('top', []))}))\n",
    "df_banglore_specs = json_normalize(df[\"new_car_specs\"].apply(lambda x: {item['key']: item['value'] for item in x.get('top', [])}))\n",
    "\n",
    "# Add city name\n",
    "for d in [df_banglore_detail, df_banglore_overview, df_banglore_feature, df_banglore_specs]:\n",
    "    d[\"City\"] = \"bangalore\"\n",
    "\n",
    "# Combine all structured data\n",
    "df_final = pd.concat([df_banglore_detail, df_banglore_overview, df_banglore_feature, df_banglore_specs], axis=1)\n",
    "\n",
    "# Save to Excel\n",
    "df_final.to_excel(\"structured_bangalore_cars.xlsx\", index=False)\n",
    "\n",
    "print(\"Data has been structured and saved to 'structured_bangalore_cars.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data has been structured and saved to 'structured_bangalore_cars.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"csv_files/bangalore_cars.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"bangalore_cars.csv\")\n",
    "\n",
    "# Function to convert string to dictionary\n",
    "def parse_dict_column(column):\n",
    "    return column.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Parse dictionary-like columns\n",
    "df[\"new_car_detail\"] = parse_dict_column(df[\"new_car_detail\"])\n",
    "df[\"new_car_overview\"] = parse_dict_column(df[\"new_car_overview\"])\n",
    "df[\"new_car_feature\"] = parse_dict_column(df[\"new_car_feature\"])\n",
    "df[\"new_car_specs\"] = parse_dict_column(df[\"new_car_specs\"])\n",
    "\n",
    "# --- Detail flattening ---\n",
    "df_banglore_detail = json_normalize(df[\"new_car_detail\"])\n",
    "\n",
    "# --- Overview flattening ---\n",
    "def flatten_overview(overview_data):\n",
    "    return {item['key']: item['value'] for item in overview_data.get(\"top\", []) if isinstance(item, dict)}\n",
    "\n",
    "df_banglore_overview = pd.json_normalize(df[\"new_car_overview\"].apply(flatten_overview))\n",
    "\n",
    "# --- Feature flattening (flatten all nested lists) ---\n",
    "def flatten_feature_list(feature_data):\n",
    "    all_features = []\n",
    "    if isinstance(feature_data, dict):\n",
    "        for section in feature_data.values():  # loop over 'top', 'others', etc.\n",
    "            if isinstance(section, list):\n",
    "                all_features.extend(section)\n",
    "    return {'Feature_' + str(i): f.get('value') for i, f in enumerate(all_features)}\n",
    "\n",
    "df_banglore_feature = pd.json_normalize(df[\"new_car_feature\"].apply(flatten_feature_list))\n",
    "\n",
    "# --- Specs flattening (flatten all nested lists) ---\n",
    "def flatten_specs_list(specs_data):\n",
    "    all_specs = []\n",
    "    if isinstance(specs_data, dict):\n",
    "        for section in specs_data.values():\n",
    "            if isinstance(section, list):\n",
    "                all_specs.extend(section)\n",
    "    return {spec.get('key'): spec.get('value') for spec in all_specs if isinstance(spec, dict) and 'key' in spec and 'value' in spec}\n",
    "\n",
    "df_banglore_specs = pd.json_normalize(df[\"new_car_specs\"].apply(flatten_specs_list))\n",
    "\n",
    "# --- Combine all sections ---\n",
    "df_final = pd.concat([df_banglore_detail, df_banglore_overview, df_banglore_feature, df_banglore_specs], axis=1)\n",
    "\n",
    "# --- Add city only once ---\n",
    "df_final[\"City\"] = \"bangalore\"\n",
    "\n",
    "# --- Save to Excel ---\n",
    "df_final.to_excel(\"structured_bangalore_cars.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ Data has been structured and saved to 'structured_bangalore_cars.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All nested features and specs parsed successfully and saved to 'structured_bangalore_cars.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"csv_files/bangalore_cars.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"bangalore_cars.csv\")\n",
    "\n",
    "# Convert stringified dictionaries to actual dicts\n",
    "def parse_dict_column(column):\n",
    "    return column.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "df[\"new_car_detail\"] = parse_dict_column(df[\"new_car_detail\"])\n",
    "df[\"new_car_overview\"] = parse_dict_column(df[\"new_car_overview\"])\n",
    "df[\"new_car_feature\"] = parse_dict_column(df[\"new_car_feature\"])\n",
    "df[\"new_car_specs\"] = parse_dict_column(df[\"new_car_specs\"])\n",
    "\n",
    "# ------------------------\n",
    "# OVERVIEW\n",
    "# ------------------------\n",
    "def flatten_overview(overview_data):\n",
    "    if isinstance(overview_data, dict):\n",
    "        return {item['key']: item['value'] for item in overview_data.get(\"top\", []) if isinstance(item, dict)}\n",
    "    return {}\n",
    "\n",
    "df_banglore_overview = pd.json_normalize(df[\"new_car_overview\"].apply(flatten_overview))\n",
    "\n",
    "# ------------------------\n",
    "# DETAIL\n",
    "# ------------------------\n",
    "df_banglore_detail = json_normalize(df[\"new_car_detail\"])\n",
    "\n",
    "# ------------------------\n",
    "# FEATURE: top + data[*].list\n",
    "# ------------------------\n",
    "def flatten_feature_all(data):\n",
    "    features = []\n",
    "    \n",
    "    # Collect top values\n",
    "    if isinstance(data, dict):\n",
    "        top = data.get(\"top\", [])\n",
    "        features.extend(f.get(\"value\") for f in top if isinstance(f, dict))\n",
    "        \n",
    "        # Collect from data section\n",
    "        for section in data.get(\"data\", []):\n",
    "            if isinstance(section, dict):\n",
    "                for item in section.get(\"list\", []):\n",
    "                    if isinstance(item, dict):\n",
    "                        val = item.get(\"value\")\n",
    "                        if val:\n",
    "                            features.append(val)\n",
    "\n",
    "    # Return as key-value Feature_0, Feature_1...\n",
    "    return {'Feature_' + str(i): v for i, v in enumerate(features)}\n",
    "\n",
    "df_banglore_feature = pd.json_normalize(df[\"new_car_feature\"].apply(flatten_feature_all))\n",
    "\n",
    "# ------------------------\n",
    "# SPECS: top + data[*].list (key-value pairs)\n",
    "# ------------------------\n",
    "def flatten_specs_all(data):\n",
    "    specs = {}\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        # Top section\n",
    "        for item in data.get(\"top\", []):\n",
    "            if isinstance(item, dict):\n",
    "                k = item.get(\"key\")\n",
    "                v = item.get(\"value\")\n",
    "                if k and v:\n",
    "                    specs[k] = v\n",
    "\n",
    "        # From data -> list\n",
    "        for section in data.get(\"data\", []):\n",
    "            if isinstance(section, dict):\n",
    "                for item in section.get(\"list\", []):\n",
    "                    if isinstance(item, dict):\n",
    "                        k = item.get(\"key\")\n",
    "                        v = item.get(\"value\")\n",
    "                        if k and v:\n",
    "                            specs[k] = v\n",
    "    return specs\n",
    "\n",
    "df_banglore_specs = pd.json_normalize(df[\"new_car_specs\"].apply(flatten_specs_all))\n",
    "\n",
    "# ------------------------\n",
    "# COMBINE ALL & ADD CITY ONCE\n",
    "# ------------------------\n",
    "df_final = pd.concat([df_banglore_detail, df_banglore_overview, df_banglore_feature, df_banglore_specs], axis=1)\n",
    "df_final[\"City\"] = \"bangalore\"\n",
    "\n",
    "# ------------------------\n",
    "# SAVE TO EXCEL\n",
    "# ------------------------\n",
    "df_final.to_excel(\"structured_bangalore_cars.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ All nested features and specs parsed successfully and saved to 'structured_bangalore_cars.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sheets: ['Sheet1']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\USER\\Desktop\\Car-Dheko_Used_Car_Price_Prediction\\structured_bangalore_cars.xlsx\"\n",
    "\n",
    "# Get all sheet names\n",
    "sheets = pd.ExcelFile(file_path).sheet_names\n",
    "print(\"Available sheets:\", sheets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   it      ft         bt        km transmission  ownerNo      owner      oem  \\\n",
      "0   0  Petrol  Hatchback  1,20,000       Manual        3  3rd Owner   Maruti   \n",
      "1   0  Petrol        SUV    32,706       Manual        2  2nd Owner     Ford   \n",
      "2   0  Petrol  Hatchback    11,949       Manual        1  1st Owner     Tata   \n",
      "3   0  Petrol      Sedan    17,794       Manual        1  1st Owner  Hyundai   \n",
      "4   0  Diesel        SUV    60,000       Manual        1  1st Owner   Maruti   \n",
      "\n",
      "                model  modelYear  ...  Feature_11 Feature_12 Feature_13  \\\n",
      "0      Maruti Celerio       2015  ...         NaN        NaN        NaN   \n",
      "1       Ford Ecosport       2018  ...         NaN        NaN        NaN   \n",
      "2          Tata Tiago       2018  ...         NaN        NaN        NaN   \n",
      "3       Hyundai Xcent       2014  ...         NaN        NaN        NaN   \n",
      "4  Maruti SX4 S Cross       2015  ...         NaN        NaN        NaN   \n",
      "\n",
      "      Mileage   Engine  Max Power    Torque Seats.1 Wheel Size       City  \n",
      "0   23.1 kmpl   998 CC   67.04bhp      90Nm     5.0        NaN  bangalore  \n",
      "1     17 kmpl  1497 CC  121.31bhp     150Nm     5.0         16  bangalore  \n",
      "2  23.84 kmpl  1199 CC      84bhp     114Nm     5.0         14  bangalore  \n",
      "3   19.1 kmpl  1197 CC   81.86bhp  113.75Nm     5.0         14  bangalore  \n",
      "4  23.65 kmpl  1248 CC    88.5bhp     200Nm     5.0         16  bangalore  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_excel(file_path, sheet_name=\"Sheet1\")  # or sheet_name=0 for the first sheet\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'ft', 'bt', 'km', 'transmission', 'ownerNo', 'owner', 'oem', 'model', 'modelYear', 'centralVariantId', 'variantName', 'price', 'priceActual', 'priceSaving', 'priceFixedText', 'trendingText.imgUrl', 'trendingText.heading', 'trendingText.desc', 'Registration Year', 'Insurance Validity', 'Fuel Type', 'Seats', 'Kms Driven', 'RTO', 'Ownership', 'Engine Displacement', 'Transmission', 'Year of Manufacture', 'Feature_0', 'Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5', 'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_10', 'Feature_11', 'Feature_12', 'Feature_13', 'Mileage', 'Engine', 'Max Power', 'Torque', 'Seats.1', 'Wheel Size', 'City']\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1481 entries, 0 to 1480\n",
      "Data columns (total 50 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   it                    1481 non-null   int64  \n",
      " 1   ft                    1481 non-null   object \n",
      " 2   bt                    1481 non-null   object \n",
      " 3   km                    1481 non-null   object \n",
      " 4   transmission          1481 non-null   object \n",
      " 5   ownerNo               1481 non-null   int64  \n",
      " 6   owner                 1481 non-null   object \n",
      " 7   oem                   1481 non-null   object \n",
      " 8   model                 1481 non-null   object \n",
      " 9   modelYear             1481 non-null   int64  \n",
      " 10  centralVariantId      1481 non-null   int64  \n",
      " 11  variantName           1481 non-null   object \n",
      " 12  price                 1481 non-null   object \n",
      " 13  priceActual           311 non-null    object \n",
      " 14  priceSaving           0 non-null      float64\n",
      " 15  priceFixedText        0 non-null      float64\n",
      " 16  trendingText.imgUrl   1481 non-null   object \n",
      " 17  trendingText.heading  1481 non-null   object \n",
      " 18  trendingText.desc     1481 non-null   object \n",
      " 19  Registration Year     1474 non-null   object \n",
      " 20  Insurance Validity    1478 non-null   object \n",
      " 21  Fuel Type             1481 non-null   object \n",
      " 22  Seats                 1480 non-null   object \n",
      " 23  Kms Driven            1481 non-null   object \n",
      " 24  RTO                   1313 non-null   object \n",
      " 25  Ownership             1481 non-null   object \n",
      " 26  Engine Displacement   1478 non-null   object \n",
      " 27  Transmission          1481 non-null   object \n",
      " 28  Year of Manufacture   1474 non-null   float64\n",
      " 29  Feature_0             1467 non-null   object \n",
      " 30  Feature_1             1467 non-null   object \n",
      " 31  Feature_2             1464 non-null   object \n",
      " 32  Feature_3             1464 non-null   object \n",
      " 33  Feature_4             1461 non-null   object \n",
      " 34  Feature_5             1460 non-null   object \n",
      " 35  Feature_6             1457 non-null   object \n",
      " 36  Feature_7             1455 non-null   object \n",
      " 37  Feature_8             1345 non-null   object \n",
      " 38  Feature_9             0 non-null      float64\n",
      " 39  Feature_10            0 non-null      float64\n",
      " 40  Feature_11            0 non-null      float64\n",
      " 41  Feature_12            0 non-null      float64\n",
      " 42  Feature_13            0 non-null      float64\n",
      " 43  Mileage               1439 non-null   object \n",
      " 44  Engine                1478 non-null   object \n",
      " 45  Max Power             1465 non-null   object \n",
      " 46  Torque                1465 non-null   object \n",
      " 47  Seats.1               1480 non-null   float64\n",
      " 48  Wheel Size            1029 non-null   object \n",
      " 49  City                  1481 non-null   object \n",
      "dtypes: float64(9), int64(4), object(37)\n",
      "memory usage: 578.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Excel files in 'csv_files': ['csv_files\\\\bangalore_cars.xlsx', 'csv_files\\\\chennai_cars.xlsx', 'csv_files\\\\city_files_summary.xlsx', 'csv_files\\\\delhi_cars.xlsx', 'csv_files\\\\hyderabad_cars.xlsx', 'csv_files\\\\jaipur_cars.xlsx', 'csv_files\\\\kolkata_cars.xlsx']\n",
      "\n",
      "📊 File Summary:\n",
      "                 City     File_Type File_Size  Rows  Columns\n",
      "0      bangalore_cars  Excel (XLSX)   0.55 MB  1481        5\n",
      "1        chennai_cars  Excel (XLSX)   0.54 MB  1419        5\n",
      "2  city_files_summary  Excel (XLSX)   0.01 MB     6        7\n",
      "3          delhi_cars  Excel (XLSX)   0.56 MB  1485        5\n",
      "4      hyderabad_cars  Excel (XLSX)   0.55 MB  1483        5\n",
      "5         jaipur_cars  Excel (XLSX)    0.4 MB  1120        5\n",
      "6        kolkata_cars  Excel (XLSX)   0.51 MB  1381        5\n",
      "\n",
      "✅ Summary saved to: csv_files\\city_files_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# 1. Specify the correct folder path (where your Excel files are stored)\n",
    "folder_path = \"csv_files\"  # Replace with your actual folder name if different\n",
    "\n",
    "# 2. List all city Excel files in the folder\n",
    "city_files = glob(os.path.join(folder_path, '*.xlsx'))  # Searches inside 'csv_files' folder\n",
    "print(f\"Found Excel files in '{folder_path}':\", city_files)\n",
    "\n",
    "if not city_files:\n",
    "    print(f\"\\n❌ No Excel files found in '{folder_path}'. Please check:\")\n",
    "    print(\"- Folder name is correct (case-sensitive)\")\n",
    "    print(\"- Files have '.xlsx' extension\")\n",
    "    print(\"- Files are not in a sub-subfolder\")\n",
    "else:\n",
    "    # 3. Create a summary DataFrame\n",
    "    summary_data = []\n",
    "\n",
    "    for file in city_files:\n",
    "        city_name = os.path.splitext(os.path.basename(file))[0]  # Extract filename without path/extension\n",
    "        try:\n",
    "            # Read first 2 rows to check structure\n",
    "            df_sample = pd.read_excel(file, nrows=2)\n",
    "            \n",
    "            # Get file info\n",
    "            file_size = f\"{round(os.path.getsize(file)/(1024*1024), 2)} MB\"\n",
    "            file_columns = df_sample.columns.tolist()\n",
    "            file_shape = pd.read_excel(file).shape\n",
    "            \n",
    "            summary_data.append({\n",
    "                'City': city_name,\n",
    "                'File_Type': 'Excel (XLSX)',\n",
    "                'File_Size': file_size,\n",
    "                'Rows': file_shape[0],\n",
    "                'Columns': file_shape[1],\n",
    "                'Columns_List': file_columns,\n",
    "                'Sample_First_Row': df_sample.iloc[0].to_dict() if len(df_sample) > 0 else {}\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "            summary_data.append({\n",
    "                'City': city_name,\n",
    "                'Error': str(e)\n",
    "            })\n",
    "\n",
    "    # 4. Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # 5. Display results\n",
    "    if not summary_df.empty:\n",
    "        print(\"\\n📊 File Summary:\")\n",
    "        print(summary_df[['City', 'File_Type', 'File_Size', 'Rows', 'Columns']])\n",
    "        \n",
    "        # Save to Excel\n",
    "        output_path = os.path.join(folder_path, 'city_files_summary.xlsx')\n",
    "        summary_df.to_excel(output_path, index=False)\n",
    "        print(f\"\\n✅ Summary saved to: {output_path}\")\n",
    "    else:\n",
    "        print(\"\\n❌ All files failed to load. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read csv_files\\bangalore_cars.xlsx\n",
      "Columns: ['new_car_detail', 'new_car_overview', 'new_car_feature', 'new_car_specs', 'car_links']\n"
     ]
    }
   ],
   "source": [
    "# Test reading one file\n",
    "test_file = city_files[0]  # or specify a filename\n",
    "try:\n",
    "    test_df = pd.read_excel(test_file)\n",
    "    print(f\"Successfully read {test_file}\")\n",
    "    print(\"Columns:\", test_df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read {test_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Version (Checks All Sheets):\n",
    "# If your Excel files have multiple sheets, use this:\n",
    "\n",
    "# Modified loop to check all sheets\n",
    "for file in city_files:\n",
    "    city_name = os.path.splitext(file)[0]\n",
    "    try:\n",
    "        xl = pd.ExcelFile(file)  # Open Excel file\n",
    "        for sheet_name in xl.sheet_names:  # Loop through all sheets\n",
    "            df_sample = xl.parse(sheet_name, nrows=2)\n",
    "            file_size = f\"{round(os.path.getsize(file)/(1024*1024), 2)} MB\"\n",
    "            summary_data.append({\n",
    "                'City': city_name,\n",
    "                'Sheet_Name': sheet_name,\n",
    "                'File_Type': 'Excel (XLSX)',\n",
    "                'File_Size': file_size,\n",
    "                'Rows': xl.parse(sheet_name).shape[0],\n",
    "                'Columns': xl.parse(sheet_name).shape[1],\n",
    "                'Columns_List': df_sample.columns.tolist(),\n",
    "                'Dtypes': df_sample.dtypes.to_dict()\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique columns across all files: ['new_car_detail' 'new_car_overview' 'new_car_feature' 'new_car_specs'\n",
      " 'car_links' 'City' 'File_Type' 'File_Size' 'Rows' 'Columns'\n",
      " 'Columns_List' 'Sample_First_Row']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check for consistency across files:\n",
    "# Compare columns across all cities\n",
    "all_columns = summary_df['Columns_List'].explode().unique()\n",
    "print(\"Unique columns across all files:\", all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all files into a single DataFrame:\n",
    "\n",
    "all_cities = pd.concat(\n",
    "    [pd.read_excel(file).assign(City=os.path.splitext(file)[0]) \n",
    "    for file in city_files\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the Index\n",
    "all_cities = all_cities.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_car_detail         7\n",
      "new_car_overview       7\n",
      "new_car_feature        7\n",
      "new_car_specs          7\n",
      "car_links              7\n",
      "City                   0\n",
      "File_Type           8369\n",
      "File_Size           8369\n",
      "Rows                8369\n",
      "Columns             8369\n",
      "Columns_List        8369\n",
      "Sample_First_Row    8369\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check for Missing Values\n",
    "print(all_cities.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      new_car_detail  \\\n",
      "0  {'it': 0, 'ft': 'Petrol', 'bt': 'Hatchback', '...   \n",
      "1  {'it': 0, 'ft': 'Petrol', 'bt': 'SUV', 'km': '...   \n",
      "2  {'it': 0, 'ft': 'Petrol', 'bt': 'Hatchback', '...   \n",
      "3  {'it': 0, 'ft': 'Petrol', 'bt': 'Sedan', 'km':...   \n",
      "4  {'it': 0, 'ft': 'Diesel', 'bt': 'SUV', 'km': '...   \n",
      "\n",
      "                                    new_car_overview  \\\n",
      "0  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "1  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "2  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "3  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "4  {'heading': 'Car overview', 'top': [{'key': 'R...   \n",
      "\n",
      "                                     new_car_feature  \\\n",
      "0  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "1  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "2  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "3  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "4  {'heading': 'Features', 'top': [{'value': 'Pow...   \n",
      "\n",
      "                                       new_car_specs  \\\n",
      "0  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "1  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "2  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "3  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "4  {'heading': 'Specifications', 'top': [{'key': ...   \n",
      "\n",
      "                                           car_links  \\\n",
      "0  https://www.cardekho.com/used-car-details/used...   \n",
      "1  https://www.cardekho.com/buy-used-car-details/...   \n",
      "2  https://www.cardekho.com/used-car-details/used...   \n",
      "3  https://www.cardekho.com/buy-used-car-details/...   \n",
      "4  https://www.cardekho.com/used-car-details/used...   \n",
      "\n",
      "                       City File_Type File_Size  Rows  Columns Columns_List  \\\n",
      "0  csv_files\\bangalore_cars       NaN       NaN   NaN      NaN          NaN   \n",
      "1  csv_files\\bangalore_cars       NaN       NaN   NaN      NaN          NaN   \n",
      "2  csv_files\\bangalore_cars       NaN       NaN   NaN      NaN          NaN   \n",
      "3  csv_files\\bangalore_cars       NaN       NaN   NaN      NaN          NaN   \n",
      "4  csv_files\\bangalore_cars       NaN       NaN   NaN      NaN          NaN   \n",
      "\n",
      "  Sample_First_Row  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "3              NaN  \n",
      "4              NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8376 entries, 0 to 8375\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   new_car_detail    8369 non-null   object \n",
      " 1   new_car_overview  8369 non-null   object \n",
      " 2   new_car_feature   8369 non-null   object \n",
      " 3   new_car_specs     8369 non-null   object \n",
      " 4   car_links         8369 non-null   object \n",
      " 5   City              8376 non-null   object \n",
      " 6   File_Type         7 non-null      object \n",
      " 7   File_Size         7 non-null      object \n",
      " 8   Rows              7 non-null      float64\n",
      " 9   Columns           7 non-null      float64\n",
      " 10  Columns_List      7 non-null      object \n",
      " 11  Sample_First_Row  7 non-null      object \n",
      "dtypes: float64(2), object(10)\n",
      "memory usage: 785.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(all_cities.head())\n",
    "print(all_cities.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cities.columns = all_cities.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "#Makes column names consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Handle Data Type Issues  Convert columns to appropriate data types.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_cities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_column\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mall_cities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date_column'"
     ]
    }
   ],
   "source": [
    "#Handle Data Type Issues  Convert columns to appropriate data types.\n",
    "all_cities['date_column'] = pd.to_datetime(all_cities['date_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_car_detail', 'new_car_overview', 'new_car_feature', 'new_car_specs', 'car_links', 'city', 'file_type', 'file_size', 'rows', 'columns', 'columns_list', 'sample_first_row']\n"
     ]
    }
   ],
   "source": [
    "print(all_cities.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_cities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mall_cities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# converts errors to NaT\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "all_cities['date'] = pd.to_datetime(all_cities['date'], errors='coerce')  # converts errors to NaT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Import and Structuring**\n",
    "\n",
    "**a) Import and Concatenate City Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[0;32m     52\u001b[0m data_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/your/city/files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 53\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_cities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mprocess_all_cities\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m     44\u001b[0m     all_cities\u001b[38;5;241m.\u001b[39mappend(city_df)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Concatenate all city DataFrames\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_cities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_df\n",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\python\\python36\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "def process_city_file(file_path, city_name):\n",
    "    \"\"\"Process a single city file and return structured DataFrame\"\"\"\n",
    "    df = pd.read_csv(file_path)  # or pd.read_excel() if Excel files\n",
    "    \n",
    "    # Extract nested JSON data from columns\n",
    "    for col in ['new_car_detail', 'new_car_overview', 'new_car_feature', 'new_car_specs']:\n",
    "        df[col] = df[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Flatten nested JSON structures\n",
    "    details = pd.json_normalize(df['new_car_detail'])\n",
    "    overview = pd.json_normalize(df['new_car_overview'].apply(lambda x: x['top']))\n",
    "    features = pd.json_normalize(df['new_car_feature'].apply(lambda x: [item['value'] for item in x['top']]))\n",
    "    specs = pd.json_normalize(df['new_car_specs'].apply(lambda x: x['top']))\n",
    "    \n",
    "    # Combine all data\n",
    "    processed_df = pd.concat([\n",
    "        df[['car_links']],\n",
    "        details,\n",
    "        overview,\n",
    "        features,\n",
    "        specs\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Add city column\n",
    "    processed_df['City'] = city_name\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def process_all_cities(data_dir):\n",
    "    \"\"\"Process all city files in directory\"\"\"\n",
    "    all_cities = []\n",
    "    \n",
    "    # Get all city files (assuming naming pattern like 'Chennai.csv', 'Mumbai.csv')\n",
    "    city_files = glob(os.path.join(data_dir, '*.csv'))  # or '*.xlsx' for Excel\n",
    "    \n",
    "    for file_path in city_files:\n",
    "        city_name = os.path.basename(file_path).split('.')[0]\n",
    "        city_df = process_city_file(file_path, city_name)\n",
    "        all_cities.append(city_df)\n",
    "    \n",
    "    # Concatenate all city DataFrames\n",
    "    combined_df = pd.concat(all_cities, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Usage:\n",
    "data_directory = 'path/to/your/city/files'\n",
    "combined_data = process_all_cities(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handling Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Handle missing values in the dataset\"\"\"\n",
    "    \n",
    "    # Numerical columns - fill with median\n",
    "    numerical_cols = ['km', 'modelYear', 'ownerNo', 'priceActual']\n",
    "    for col in numerical_cols:\n",
    "        if col in df.columns:\n",
    "            # Clean numerical columns first (remove commas, non-numeric chars)\n",
    "            df[col] = pd.to_numeric(df[col].astype(str).str.replace('[^\\d.]', '', regex=True), errors='coerce')\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    # Categorical columns - fill with mode or 'Unknown'\n",
    "    categorical_cols = ['ft', 'bt', 'transmission', 'owner', 'oem', 'model']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', inplace=True)\n",
    "    \n",
    "    # Price handling - extract numerical value from string\n",
    "    if 'price' in df.columns:\n",
    "        df['price'] = df['price'].str.extract(r'₹\\s*([\\d,.]+)')[0]\n",
    "        df['price'] = pd.to_numeric(df['price'].str.replace(',', ''), errors='coerce')\n",
    "        df['price'].fillna(df['price'].median(), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply missing value handling\n",
    "cleaned_data = handle_missing_values(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing Data Formats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data_formats(df):\n",
    "    \"\"\"Standardize data formats across columns\"\"\"\n",
    "    \n",
    "    # Convert kms driven to numeric\n",
    "    if 'km' in df.columns:\n",
    "        df['km'] = df['km'].astype(str).str.replace('Kms', '').str.replace(',', '').str.strip()\n",
    "        df['km'] = pd.to_numeric(df['km'], errors='coerce')\n",
    "    \n",
    "    # Convert engine displacement to numeric (cc)\n",
    "    if 'engineDisplacement' in df.columns:\n",
    "        df['engineDisplacement_cc'] = df['engineDisplacement'].str.extract(r'(\\d+)').astype(float)\n",
    "    \n",
    "    # Extract power values\n",
    "    if 'maxPower' in df.columns:\n",
    "        df['maxPower_bhp'] = df['maxPower'].str.extract(r'([\\d.]+)').astype(float)\n",
    "    \n",
    "    # Convert torque to numeric\n",
    "    if 'torque' in df.columns:\n",
    "        df['torque_nm'] = df['torque'].str.extract(r'([\\d.]+)').astype(float)\n",
    "    \n",
    "    # Convert year columns to datetime\n",
    "    year_cols = ['modelYear', 'registrationYear']\n",
    "    for col in year_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], format='%Y', errors='coerce').dt.year\n",
    "    \n",
    "    # Standardize owner information\n",
    "    if 'owner' in df.columns:\n",
    "        df['owner'] = df['owner'].str.replace(r'\\d(st|nd|rd|th)\\s*Owner', lambda m: m.group(1) + ' Owner', regex=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply standardization\n",
    "standardized_data = standardize_data_formats(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_engineering(df):\n",
    "    \"\"\"Create new features from existing data\"\"\"\n",
    "    \n",
    "    # Calculate car age\n",
    "    current_year = pd.Timestamp.now().year\n",
    "    if 'modelYear' in df.columns:\n",
    "        df['car_age'] = current_year - df['modelYear']\n",
    "    \n",
    "    # Create binary features from car features\n",
    "    if 'features' in df.columns:\n",
    "        all_features = set()\n",
    "        for feature_list in df['features']:\n",
    "            if isinstance(feature_list, list):\n",
    "                all_features.update(feature_list)\n",
    "        \n",
    "        for feature in all_features:\n",
    "            df[f'has_{feature}'] = df['features'].apply(lambda x: feature in x if isinstance(x, list) else False)\n",
    "    \n",
    "    # Create transmission type binary columns\n",
    "    if 'transmission' in df.columns:\n",
    "        df['is_automatic'] = df['transmission'].str.contains('Automatic', case=False)\n",
    "        df['is_manual'] = df['transmission'].str.contains('Manual', case=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "final_data = perform_feature_engineering(standardized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Saving Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data\n",
    "final_data.to_csv('processed_car_data.csv', index=False)\n",
    "# Or save to pickle for preserving data types\n",
    "final_data.to_pickle('processed_car_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Encoding Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def encode_categorical_variables(df):\n",
    "    \"\"\"Encode categorical variables using appropriate techniques\"\"\"\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    nominal_cats = ['oem', 'model', 'City', 'fuelType', 'transmissionType']  # No inherent ordering\n",
    "    ordinal_cats = ['owner']  # Has meaningful order (1st > 2nd > 3rd Owner)\n",
    "    \n",
    "    # One-hot encoding for nominal variables\n",
    "    onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(df[nominal_cats])\n",
    "    \n",
    "    # Create DataFrame from one-hot encoded data\n",
    "    onehot_df = pd.DataFrame(\n",
    "        onehot_encoded,\n",
    "        columns=onehot_encoder.get_feature_names_out(nominal_cats)\n",
    "    )\n",
    "    \n",
    "    # Ordinal encoding for ordered categories\n",
    "    owner_order = ['4th Owner', '3rd Owner', '2nd Owner', '1st Owner']  # Lower is better\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[owner_order])\n",
    "    df['owner_encoded'] = ordinal_encoder.fit_transform(df[['owner']])\n",
    "    \n",
    "    # Label encoding for binary categories (if any)\n",
    "    binary_cols = ['is_automatic', 'is_manual']  # Example binary columns\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = LabelEncoder().fit_transform(df[col])\n",
    "    \n",
    "    # Combine encoded data with original numerical data\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    final_df = pd.concat([\n",
    "        df[numerical_cols],\n",
    "        onehot_df,\n",
    "        df[['owner_encoded']]\n",
    "    ], axis=1)\n",
    "    \n",
    "    return final_df, onehot_encoder, ordinal_encoder\n",
    "\n",
    "# Apply encoding\n",
    "encoded_data, ohe, ord_enc = encode_categorical_variables(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Normalizing Numerical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def normalize_numerical_features(df, scaling_method='minmax'):\n",
    "    \"\"\"Normalize numerical features to common scale\"\"\"\n",
    "    \n",
    "    # Select numerical columns (excluding target and encoded columns)\n",
    "    numerical_cols = [col for col in df.columns \n",
    "                    if df[col].dtype in ['int64', 'float64'] \n",
    "                    and col != 'price'  # Exclude target variable\n",
    "                    and not col.endswith('_encoded')]\n",
    "    \n",
    "    # Apply selected scaling method\n",
    "    if scaling_method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:  # standard\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "# Apply normalization (choose either 'minmax' or 'standard')\n",
    "normalized_data, feature_scaler = normalize_numerical_features(encoded_data, 'minmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Removing Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def handle_outliers(df, method='iqr', threshold=1.5):\n",
    "    \"\"\"Identify and handle outliers in numerical features\"\"\"\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        # IQR method\n",
    "        for col in numerical_cols:\n",
    "            if col != 'price':  # Don't remove based on target variable\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "                \n",
    "                # Cap outliers instead of removing\n",
    "                df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    elif method == 'zscore':\n",
    "        # Z-score method\n",
    "        for col in numerical_cols:\n",
    "            if col != 'price':\n",
    "                z_scores = stats.zscore(df[col])\n",
    "                df = df[(np.abs(z_scores) < threshold)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply outlier handling (choose either 'iqr' or 'zscore')\n",
    "cleaned_data = handle_outliers(normalized_data, 'iqr', 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete Data Processing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_processing_pipeline(data_dir, city_files):\n",
    "    \"\"\"Complete data processing pipeline\"\"\"\n",
    "    \n",
    "    # 1. Import and concatenate city datasets\n",
    "    combined_data = process_all_cities(data_dir, city_files)\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    cleaned_data = handle_missing_values(combined_data)\n",
    "    \n",
    "    # 3. Standardize data formats\n",
    "    standardized_data = standardize_data_formats(cleaned_data)\n",
    "    \n",
    "    # 4. Feature engineering\n",
    "    final_data = perform_feature_engineering(standardized_data)\n",
    "    \n",
    "    # 5. Encode categorical variables\n",
    "    encoded_data, ohe, ord_enc = encode_categorical_variables(final_data)\n",
    "    \n",
    "    # 6. Normalize numerical features\n",
    "    normalized_data, feature_scaler = normalize_numerical_features(encoded_data)\n",
    "    \n",
    "    # 7. Handle outliers\n",
    "    processed_data = handle_outliers(normalized_data)\n",
    "    \n",
    "    return processed_data, ohe, ord_enc, feature_scaler\n",
    "\n",
    "# Example usage:\n",
    "data_dir = 'path/to/city/files'\n",
    "city_files = ['Chennai.csv', 'Mumbai.csv', 'Delhi.csv']  # etc.\n",
    "final_processed_data, ohe, ord_enc, scaler = full_data_processing_pipeline(data_dir, city_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
