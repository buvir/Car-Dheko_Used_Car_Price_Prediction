{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Processing for Car-Dheko Used Car Price Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Excel files: []\n",
      "\n",
      "Available columns in summary_df: []\n",
      "\n",
      "No valid data to display. All files failed to load.\n",
      "\n",
      "No data to save - all files failed to load.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# 1. Specify the correct folder path (where your Excel files are stored)\n",
    "folder_path = \"csv_files\"  # Replace with your actual folder name if different\n",
    "\n",
    "# 2. List all city Excel files in the folder\n",
    "city_files = glob(os.path.join(folder_path, '*.xlsx'))  # Searches inside 'csv_files' folder\n",
    "print(f\"Found Excel files in '{folder_path}':\", city_files)\n",
    "\n",
    "if not city_files:\n",
    "    print(f\"\\n‚ùå No Excel files found in '{folder_path}'. Please check:\")\n",
    "    print(\"- Folder name is correct (case-sensitive)\")\n",
    "    print(\"- Files have '.xlsx' extension\")\n",
    "    print(\"- Files are not in a sub-subfolder\")\n",
    "else:\n",
    "    # 3. Create a summary DataFrame\n",
    "    summary_data = []\n",
    "\n",
    "    for file in city_files:\n",
    "        city_name = os.path.splitext(os.path.basename(file))[0]  # Extract filename without path/extension\n",
    "        try:\n",
    "            # Read first 2 rows to check structure\n",
    "            df_sample = pd.read_excel(file, nrows=2)\n",
    "            \n",
    "            # Get file info\n",
    "            file_size = f\"{round(os.path.getsize(file)/(1024*1024), 2)} MB\"\n",
    "            file_columns = df_sample.columns.tolist()\n",
    "            file_shape = pd.read_excel(file).shape\n",
    "            \n",
    "            summary_data.append({\n",
    "                'City': city_name,\n",
    "                'File_Type': 'Excel (XLSX)',\n",
    "                'File_Size': file_size,\n",
    "                'Rows': file_shape[0],\n",
    "                'Columns': file_shape[1],\n",
    "                'Columns_List': file_columns,\n",
    "                'Sample_First_Row': df_sample.iloc[0].to_dict() if len(df_sample) > 0 else {}\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "            summary_data.append({\n",
    "                'City': city_name,\n",
    "                'Error': str(e)\n",
    "            })\n",
    "\n",
    "    # 4. Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # 5. Display results\n",
    "    if not summary_df.empty:\n",
    "        print(\"\\nüìä File Summary:\")\n",
    "        print(summary_df[['City', 'File_Type', 'File_Size', 'Rows', 'Columns']])\n",
    "        \n",
    "        # Save to Excel\n",
    "        output_path = os.path.join(folder_path, 'city_files_summary.xlsx')\n",
    "        summary_df.to_excel(output_path, index=False)\n",
    "        print(f\"\\n‚úÖ Summary saved to: {output_path}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå All files failed to load. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading one file\n",
    "test_file = city_files[0]  # or specify a filename\n",
    "try:\n",
    "    test_df = pd.read_excel(test_file)\n",
    "    print(f\"Successfully read {test_file}\")\n",
    "    print(\"Columns:\", test_df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read {test_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Version (Checks All Sheets):\n",
    "# If your Excel files have multiple sheets, use this:\n",
    "\n",
    "# Modified loop to check all sheets\n",
    "for file in city_files:\n",
    "    city_name = os.path.splitext(file)[0]\n",
    "    try:\n",
    "        xl = pd.ExcelFile(file)  # Open Excel file\n",
    "        for sheet_name in xl.sheet_names:  # Loop through all sheets\n",
    "            df_sample = xl.parse(sheet_name, nrows=2)\n",
    "            file_size = f\"{round(os.path.getsize(file)/(1024*1024), 2)} MB\"\n",
    "            summary_data.append({\n",
    "                'City': city_name,\n",
    "                'Sheet_Name': sheet_name,\n",
    "                'File_Type': 'Excel (XLSX)',\n",
    "                'File_Size': file_size,\n",
    "                'Rows': xl.parse(sheet_name).shape[0],\n",
    "                'Columns': xl.parse(sheet_name).shape[1],\n",
    "                'Columns_List': df_sample.columns.tolist(),\n",
    "                'Dtypes': df_sample.dtypes.to_dict()\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for consistency across files:\n",
    "# Compare columns across all cities\n",
    "all_columns = summary_df['Columns_List'].explode().unique()\n",
    "print(\"Unique columns across all files:\", all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all files into a single DataFrame:\n",
    "\n",
    "all_cities = pd.concat(\n",
    "    [pd.read_excel(file).assign(City=os.path.splitext(file)[0]) \n",
    "    for file in city_files\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Import and Structuring**\n",
    "\n",
    "**a) Import and Concatenate City Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "def process_city_file(file_path, city_name):\n",
    "    \"\"\"Process a single city file and return structured DataFrame\"\"\"\n",
    "    df = pd.read_csv(file_path)  # or pd.read_excel() if Excel files\n",
    "    \n",
    "    # Extract nested JSON data from columns\n",
    "    for col in ['new_car_detail', 'new_car_overview', 'new_car_feature', 'new_car_specs']:\n",
    "        df[col] = df[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Flatten nested JSON structures\n",
    "    details = pd.json_normalize(df['new_car_detail'])\n",
    "    overview = pd.json_normalize(df['new_car_overview'].apply(lambda x: x['top']))\n",
    "    features = pd.json_normalize(df['new_car_feature'].apply(lambda x: [item['value'] for item in x['top']]))\n",
    "    specs = pd.json_normalize(df['new_car_specs'].apply(lambda x: x['top']))\n",
    "    \n",
    "    # Combine all data\n",
    "    processed_df = pd.concat([\n",
    "        df[['car_links']],\n",
    "        details,\n",
    "        overview,\n",
    "        features,\n",
    "        specs\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Add city column\n",
    "    processed_df['City'] = city_name\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def process_all_cities(data_dir):\n",
    "    \"\"\"Process all city files in directory\"\"\"\n",
    "    all_cities = []\n",
    "    \n",
    "    # Get all city files (assuming naming pattern like 'Chennai.csv', 'Mumbai.csv')\n",
    "    city_files = glob(os.path.join(data_dir, '*.csv'))  # or '*.xlsx' for Excel\n",
    "    \n",
    "    for file_path in city_files:\n",
    "        city_name = os.path.basename(file_path).split('.')[0]\n",
    "        city_df = process_city_file(file_path, city_name)\n",
    "        all_cities.append(city_df)\n",
    "    \n",
    "    # Concatenate all city DataFrames\n",
    "    combined_df = pd.concat(all_cities, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Usage:\n",
    "data_directory = 'path/to/your/city/files'\n",
    "combined_data = process_all_cities(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handling Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Handle missing values in the dataset\"\"\"\n",
    "    \n",
    "    # Numerical columns - fill with median\n",
    "    numerical_cols = ['km', 'modelYear', 'ownerNo', 'priceActual']\n",
    "    for col in numerical_cols:\n",
    "        if col in df.columns:\n",
    "            # Clean numerical columns first (remove commas, non-numeric chars)\n",
    "            df[col] = pd.to_numeric(df[col].astype(str).str.replace('[^\\d.]', '', regex=True), errors='coerce')\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    # Categorical columns - fill with mode or 'Unknown'\n",
    "    categorical_cols = ['ft', 'bt', 'transmission', 'owner', 'oem', 'model']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', inplace=True)\n",
    "    \n",
    "    # Price handling - extract numerical value from string\n",
    "    if 'price' in df.columns:\n",
    "        df['price'] = df['price'].str.extract(r'‚Çπ\\s*([\\d,.]+)')[0]\n",
    "        df['price'] = pd.to_numeric(df['price'].str.replace(',', ''), errors='coerce')\n",
    "        df['price'].fillna(df['price'].median(), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply missing value handling\n",
    "cleaned_data = handle_missing_values(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing Data Formats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data_formats(df):\n",
    "    \"\"\"Standardize data formats across columns\"\"\"\n",
    "    \n",
    "    # Convert kms driven to numeric\n",
    "    if 'km' in df.columns:\n",
    "        df['km'] = df['km'].astype(str).str.replace('Kms', '').str.replace(',', '').str.strip()\n",
    "        df['km'] = pd.to_numeric(df['km'], errors='coerce')\n",
    "    \n",
    "    # Convert engine displacement to numeric (cc)\n",
    "    if 'engineDisplacement' in df.columns:\n",
    "        df['engineDisplacement_cc'] = df['engineDisplacement'].str.extract(r'(\\d+)').astype(float)\n",
    "    \n",
    "    # Extract power values\n",
    "    if 'maxPower' in df.columns:\n",
    "        df['maxPower_bhp'] = df['maxPower'].str.extract(r'([\\d.]+)').astype(float)\n",
    "    \n",
    "    # Convert torque to numeric\n",
    "    if 'torque' in df.columns:\n",
    "        df['torque_nm'] = df['torque'].str.extract(r'([\\d.]+)').astype(float)\n",
    "    \n",
    "    # Convert year columns to datetime\n",
    "    year_cols = ['modelYear', 'registrationYear']\n",
    "    for col in year_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], format='%Y', errors='coerce').dt.year\n",
    "    \n",
    "    # Standardize owner information\n",
    "    if 'owner' in df.columns:\n",
    "        df['owner'] = df['owner'].str.replace(r'\\d(st|nd|rd|th)\\s*Owner', lambda m: m.group(1) + ' Owner', regex=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply standardization\n",
    "standardized_data = standardize_data_formats(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_engineering(df):\n",
    "    \"\"\"Create new features from existing data\"\"\"\n",
    "    \n",
    "    # Calculate car age\n",
    "    current_year = pd.Timestamp.now().year\n",
    "    if 'modelYear' in df.columns:\n",
    "        df['car_age'] = current_year - df['modelYear']\n",
    "    \n",
    "    # Create binary features from car features\n",
    "    if 'features' in df.columns:\n",
    "        all_features = set()\n",
    "        for feature_list in df['features']:\n",
    "            if isinstance(feature_list, list):\n",
    "                all_features.update(feature_list)\n",
    "        \n",
    "        for feature in all_features:\n",
    "            df[f'has_{feature}'] = df['features'].apply(lambda x: feature in x if isinstance(x, list) else False)\n",
    "    \n",
    "    # Create transmission type binary columns\n",
    "    if 'transmission' in df.columns:\n",
    "        df['is_automatic'] = df['transmission'].str.contains('Automatic', case=False)\n",
    "        df['is_manual'] = df['transmission'].str.contains('Manual', case=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "final_data = perform_feature_engineering(standardized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Saving Processed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data\n",
    "final_data.to_csv('processed_car_data.csv', index=False)\n",
    "# Or save to pickle for preserving data types\n",
    "final_data.to_pickle('processed_car_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Encoding Categorical Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def encode_categorical_variables(df):\n",
    "    \"\"\"Encode categorical variables using appropriate techniques\"\"\"\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    nominal_cats = ['oem', 'model', 'City', 'fuelType', 'transmissionType']  # No inherent ordering\n",
    "    ordinal_cats = ['owner']  # Has meaningful order (1st > 2nd > 3rd Owner)\n",
    "    \n",
    "    # One-hot encoding for nominal variables\n",
    "    onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(df[nominal_cats])\n",
    "    \n",
    "    # Create DataFrame from one-hot encoded data\n",
    "    onehot_df = pd.DataFrame(\n",
    "        onehot_encoded,\n",
    "        columns=onehot_encoder.get_feature_names_out(nominal_cats)\n",
    "    )\n",
    "    \n",
    "    # Ordinal encoding for ordered categories\n",
    "    owner_order = ['4th Owner', '3rd Owner', '2nd Owner', '1st Owner']  # Lower is better\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[owner_order])\n",
    "    df['owner_encoded'] = ordinal_encoder.fit_transform(df[['owner']])\n",
    "    \n",
    "    # Label encoding for binary categories (if any)\n",
    "    binary_cols = ['is_automatic', 'is_manual']  # Example binary columns\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = LabelEncoder().fit_transform(df[col])\n",
    "    \n",
    "    # Combine encoded data with original numerical data\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    final_df = pd.concat([\n",
    "        df[numerical_cols],\n",
    "        onehot_df,\n",
    "        df[['owner_encoded']]\n",
    "    ], axis=1)\n",
    "    \n",
    "    return final_df, onehot_encoder, ordinal_encoder\n",
    "\n",
    "# Apply encoding\n",
    "encoded_data, ohe, ord_enc = encode_categorical_variables(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Normalizing Numerical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def normalize_numerical_features(df, scaling_method='minmax'):\n",
    "    \"\"\"Normalize numerical features to common scale\"\"\"\n",
    "    \n",
    "    # Select numerical columns (excluding target and encoded columns)\n",
    "    numerical_cols = [col for col in df.columns \n",
    "                    if df[col].dtype in ['int64', 'float64'] \n",
    "                    and col != 'price'  # Exclude target variable\n",
    "                    and not col.endswith('_encoded')]\n",
    "    \n",
    "    # Apply selected scaling method\n",
    "    if scaling_method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:  # standard\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "# Apply normalization (choose either 'minmax' or 'standard')\n",
    "normalized_data, feature_scaler = normalize_numerical_features(encoded_data, 'minmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Removing Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def handle_outliers(df, method='iqr', threshold=1.5):\n",
    "    \"\"\"Identify and handle outliers in numerical features\"\"\"\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        # IQR method\n",
    "        for col in numerical_cols:\n",
    "            if col != 'price':  # Don't remove based on target variable\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "                \n",
    "                # Cap outliers instead of removing\n",
    "                df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    elif method == 'zscore':\n",
    "        # Z-score method\n",
    "        for col in numerical_cols:\n",
    "            if col != 'price':\n",
    "                z_scores = stats.zscore(df[col])\n",
    "                df = df[(np.abs(z_scores) < threshold)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply outlier handling (choose either 'iqr' or 'zscore')\n",
    "cleaned_data = handle_outliers(normalized_data, 'iqr', 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete Data Processing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_processing_pipeline(data_dir, city_files):\n",
    "    \"\"\"Complete data processing pipeline\"\"\"\n",
    "    \n",
    "    # 1. Import and concatenate city datasets\n",
    "    combined_data = process_all_cities(data_dir, city_files)\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    cleaned_data = handle_missing_values(combined_data)\n",
    "    \n",
    "    # 3. Standardize data formats\n",
    "    standardized_data = standardize_data_formats(cleaned_data)\n",
    "    \n",
    "    # 4. Feature engineering\n",
    "    final_data = perform_feature_engineering(standardized_data)\n",
    "    \n",
    "    # 5. Encode categorical variables\n",
    "    encoded_data, ohe, ord_enc = encode_categorical_variables(final_data)\n",
    "    \n",
    "    # 6. Normalize numerical features\n",
    "    normalized_data, feature_scaler = normalize_numerical_features(encoded_data)\n",
    "    \n",
    "    # 7. Handle outliers\n",
    "    processed_data = handle_outliers(normalized_data)\n",
    "    \n",
    "    return processed_data, ohe, ord_enc, feature_scaler\n",
    "\n",
    "# Example usage:\n",
    "data_dir = 'path/to/city/files'\n",
    "city_files = ['Chennai.csv', 'Mumbai.csv', 'Delhi.csv']  # etc.\n",
    "final_processed_data, ohe, ord_enc, scaler = full_data_processing_pipeline(data_dir, city_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
